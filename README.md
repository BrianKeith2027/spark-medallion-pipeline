# spark-medallion-pipeline
Medallion Architecture data pipeline (Bronze â†’ Silver â†’ Gold) demonstrating lakehouse patterns for data engineering
# ğŸ… Medallion Architecture Pipeline

A data engineering pipeline implementing the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) pattern used in modern data lakehouses like Databricks and Delta Lake.

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-Data-150458?style=flat&logo=pandas&logoColor=white)
![Parquet](https://img.shields.io/badge/Parquet-Storage-50ABF1?style=flat)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## ğŸ“‹ Overview

The Medallion Architecture is a data design pattern used to organize data in a lakehouse. This project demonstrates a complete implementation with:

- **ğŸ¥‰ Bronze Layer:** Raw data ingestion with metadata tracking
- **ğŸ¥ˆ Silver Layer:** Cleaned, validated, and standardized data
- **ğŸ¥‡ Gold Layer:** Business-level aggregations ready for analytics

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAKEHOUSE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   ğŸ¥‰ BRONZE  â”‚    â”‚   ğŸ¥ˆ SILVER  â”‚    â”‚   ğŸ¥‡ GOLD    â”‚     â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚     â”‚
â”‚  â”‚ Raw Data    â”‚â”€â”€â”€â–¶â”‚ Cleaned     â”‚â”€â”€â”€â–¶â”‚ Aggregated  â”‚     â”‚
â”‚  â”‚ + Metadata  â”‚    â”‚ Validated   â”‚    â”‚ Analytics   â”‚     â”‚
â”‚  â”‚             â”‚    â”‚ Standardizedâ”‚    â”‚ Ready       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â”‚  â€¢ Parquet format   â€¢ Data quality    â€¢ Daily summary      â”‚
â”‚  â€¢ Source tracking  â€¢ Type conversion â€¢ Customer metrics   â”‚
â”‚  â€¢ Ingestion time   â€¢ Deduplication   â€¢ Product metrics    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Features

| Layer | Transformations |
|-------|-----------------|
| **ğŸ¥‰ Bronze** | Raw ingestion, metadata tagging, source tracking |
| **ğŸ¥ˆ Silver** | Column standardization, date parsing, null handling, calculated fields |
| **ğŸ¥‡ Gold** | Daily aggregations, customer analytics, product metrics, rankings |

---

## ğŸ“Š Gold Layer Outputs

### 1. Daily Sales Summary
- Total transactions per day/region
- Gross and net revenue
- Average order value

### 2. Customer Metrics
- Total orders and spend per customer
- First/last purchase dates
- Customer spend rankings

### 3. Product Metrics
- Units sold and revenue per product
- Average pricing and discounts
- Product revenue rankings

---

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install pandas numpy pyarrow
```

### Run the Pipeline

1. Clone this repository
2. Open `spark_medallion_pipeline.ipynb` in Jupyter Notebook
3. Run all cells to execute the full pipeline

---

## ğŸ“ Project Structure

```
spark-medallion-pipeline/
â”œâ”€â”€ spark_medallion_pipeline.ipynb    # Main pipeline notebook
â”œâ”€â”€ medallion_lakehouse/              # Generated data directory
â”‚   â”œâ”€â”€ bronze/                       # Raw data layer
â”‚   â”‚   â””â”€â”€ transactions/
â”‚   â”œâ”€â”€ silver/                       # Cleaned data layer
â”‚   â”‚   â””â”€â”€ transactions/
â”‚   â””â”€â”€ gold/                         # Aggregated data layer
â”‚       â”œâ”€â”€ daily_summary/
â”‚       â”œâ”€â”€ customer_metrics/
â”‚       â””â”€â”€ product_metrics/
â”œâ”€â”€ gold_daily_summary.csv            # Exported CSV
â”œâ”€â”€ gold_customer_metrics.csv         # Exported CSV
â”œâ”€â”€ gold_product_metrics.csv          # Exported CSV
â””â”€â”€ README.md
```

---

## ğŸ”„ Data Transformations

### Bronze â†’ Silver

| Transformation | Description |
|----------------|-------------|
| Column Standardization | Uppercase customer IDs, title case products |
| Date Parsing | Handle multiple date formats (YYYY-MM-DD, MM/DD/YYYY, etc.) |
| Null Handling | Default discount_pct to 0 |
| Calculated Fields | gross_amount, discount_amount, net_amount |
| Date Parts | Extract year, month, day for partitioning |

### Silver â†’ Gold

| Aggregation | Metrics |
|-------------|---------|
| Daily Summary | Transaction count, revenue, avg order value |
| Customer Metrics | Total spend, order count, rankings |
| Product Metrics | Units sold, revenue, rankings |

---

## ğŸ“ˆ Sample Output

### Customer Metrics (Top 5)
| customer_id | total_orders | total_spend | spend_rank |
|-------------|--------------|-------------|------------|
| CUST001 | 45 | $12,543.00 | 1 |
| CUST007 | 42 | $11,892.50 | 2 |
| CUST003 | 38 | $10,234.75 | 3 |

---

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **Pandas** - Data manipulation
- **NumPy** - Numerical operations
- **Parquet** - Columnar storage format
- **Jupyter Notebook** - Interactive development

---

## ğŸ’¡ Production Considerations

In a production environment, this pipeline would be implemented with:

- **Apache Spark / PySpark** - Distributed processing at scale
- **Delta Lake** - ACID transactions, time travel, schema enforcement
- **Databricks** - Managed Spark platform with Unity Catalog
- **Airflow / Prefect** - Workflow orchestration
- **Great Expectations** - Data quality validation

---

## ğŸ”® Future Improvements

- [ ] Add incremental/CDC processing
- [ ] Implement data quality framework
- [ ] Add unit tests for transformations
- [ ] Create visualization dashboard
- [ ] Add schema evolution handling

---

## ğŸ‘¤ Author

**Brian Stratton**  
Data Engineer | AI/ML Engineer | Doctoral Researcher  
[LinkedIn](https://www.linkedin.com/in/briankstratton/) | [GitHub](https://github.com/BrianKeith2027)

---

## ğŸ“„ License

This project is licensed under the MIT License.
